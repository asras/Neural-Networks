##Junk


#####BasicRNNModule
##How to get more layers?
##If class, how to implement training when U, W are only in object? They are public!
##Have to build the total symbolic network functions. When creating the network we will 
##do this for feedforward and cost. When training we loop through each layer, pass it
##the cost function which it uses to calculate appropriate gradients. It stores the
##gradients and once we have passed through the whole network, we update the weights.''

##Since we have conceived of the input sequence as a matrix, for batching the input
##is a tensor of rank 3. Hopefully we can still do the computations in a way that
##utilizes the GPU in this OO architecture.

##What are the computationally expensive operations? Is it feeding forward either for
##calculating the output or for calculating the gradients? But in Theano we call
##T.grad(cost, W) to get dW. So if we can calculate cost in an efficient manner
##when batching we should be good.

##Getting the gradient is part of the setup of the neural network. We stitch the
##different layers together to create the total FeedForward and cost functions,
##then we pass the cost function back through the layers to let them calculate and
##save the gradient function for later updates.